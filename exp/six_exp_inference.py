"""
æ—¥å¿—ï¼š
20240530: ç”¨æ ‡å‡†çš„å»ºæ¨¡æ¶æ„
20240530ï¼šè¦å…ˆå°†ç»çº¬åº¦è¿™ç§ç»å¯¹æ•°æ®è½¬æ¢ä¸ºdis-angè¿™ç§ç›¸å¯¹æ•°æ®ï¼Œä½†è®­ç»ƒè¦ç”¨ç»çº¬åº¦æ•°æ®é‡Œé¿å…0-360Â°ç¿»è½¬çš„é—®é¢˜
20240531: å¼•å…¥åŠ æ€§æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ•ˆæœéå¸¸å¥½
20240606: LSTM+transformer+KF+è‡ªæ³¨æ„åŠ› æ•ˆæœéå¸¸å¥½
20240615: æ”¹æˆå›ºå®šå‘¨æœŸ,é‡å†™lstm_model_test
20240615: æ”¹å†™äº†å„ç§æ³¨æ„åŠ›æ¨¡å‹
20240615: æ”¹å†™æˆinformer(æ•ˆæœå¾ˆå¥½)
20240618: testé‡Œèåˆé¢„æµ‹å€¼å’Œè§‚æµ‹å€¼
20240618: testé‡Œèåˆé¢„æµ‹å€¼å’Œè§‚æµ‹å€¼diså’Œangåˆ†å¼€èåˆ
"""
import torch
import numpy as np
import csv
import sys
import torch.nn as nn
from tqdm import tqdm
from torch.utils.data import TensorDataset, DataLoader
from functions import inverse_standardize, cartesian_to_polar, haversine_distances
from matplot_func import polar_plot_actual_pred1, plot_training_losses
# ç°åœ¨å¯ä»¥å¯¼å…¥ src é‡Œçš„æ¨¡å—
from dataloaders.prepare.six.six_datasets_loader import load_data, load_small_inference_set
from sklearn.metrics import mean_squared_error, mean_absolute_error
import csv
from datetime import datetime  # ç”¨äºè·å–å½“å‰æ—¶é—´


import pandas as pd
import os
import warnings

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
warnings.filterwarnings("ignore")
import matplotlib.pyplot as plt
from datetime import datetime


import os
import csv
import numpy as np
import matplotlib.pyplot as plt


def count_parameters(model):
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Total parameters: {total_params:,}")
    print(f"Trainable parameters: {trainable_params:,}")
    return total_params, trainable_params

def visualize_best_samples(best_samples, base_samples_dir='./samples', plot_feature_idx=6):
    inputs = best_samples['inputs']   # (batch_size, seq_len_in, feature_dim)
    outputs = best_samples['outputs']  # (batch_size, seq_len_out, feature_dim)
    targets = best_samples['targets']  # (batch_size, seq_len_out, feature_dim)

    input_seq = inputs[0].cpu().numpy()   # (seq_len_in, feature_dim)
    target_seq = targets[0].cpu().numpy() # (seq_len_out, feature_dim)
    output_seq = outputs[0].cpu().numpy() # (seq_len_out, feature_dim)

    seq_len_in, feature_dim = input_seq.shape
    seq_len_out = target_seq.shape[0]

    # ================== å¯è§†åŒ–ï¼ˆåªç”»ä¸€ä¸ªç‰¹å¾ï¼‰ ==================
    real_seq_plot = np.concatenate([input_seq[:, plot_feature_idx], target_seq[:, plot_feature_idx]])
    pred_seq_plot = np.concatenate([np.full(seq_len_in, np.nan), output_seq[:, plot_feature_idx]])

    plt.figure(figsize=(12, 6))
    plt.plot(real_seq_plot, label='Real (Input + Target)', color='blue')
    plt.plot(pred_seq_plot, label='Prediction (Output)', color='orange')
    plt.legend()
    plt.title(f'Best batch example: Feature {plot_feature_idx}')
    plt.xlabel('Time step')
    plt.ylabel('Value')
    plt.tight_layout()
    plt.show()

    # ================== ä¿å­˜ä¸º CSVï¼ˆä¿å­˜æ‰€æœ‰ç‰¹å¾ï¼‰ ==================
    os.makedirs(base_samples_dir, exist_ok=True)
    csv_path = os.path.join(base_samples_dir, f'best_sequence_{inputs.shape[1]}.csv')

    with open(csv_path, 'w', newline='') as f:
        writer = csv.writer(f)
        # å†™è¡¨å¤´
        header = ['TimeStep']
        for i in range(feature_dim):
            header += [f'real_f{i}', f'pred_f{i}']
        writer.writerow(header)

        # å†™æ•°æ®è¡Œ
        for t in range(seq_len_in + seq_len_out):
            row = [t]
            for i in range(feature_dim):
                # çœŸå®å€¼
                if t < seq_len_in:
                    real_val = input_seq[t, i]
                else:
                    real_val = target_seq[t - seq_len_in, i]
                # é¢„æµ‹å€¼
                if t < seq_len_in:
                    pred_val = np.nan
                else:
                    pred_val = output_seq[t - seq_len_in, i]
                row += [real_val, pred_val]
            writer.writerow(row)

    print(f"âœ… æ‰€æœ‰ç‰¹å¾çš„æ—¶é—´åºåˆ—å·²ä¿å­˜ä¸º CSV: {csv_path}")




class EarlyStopping:
    def __init__(self, patience=5, min_delta=0.001):
        """
        æ—©åœç±»ï¼Œç”¨äºç›‘æ§éªŒè¯æŸå¤±
        Args:
            patience (int): æ²¡æœ‰æ”¹å–„çš„éªŒè¯æŸå¤±å…è®¸çš„æœ€å¤§æ¬¡æ•°
            min_delta (float): æœ€å°æ”¹å–„å€¼ï¼Œå¦‚æœå˜åŒ–å°äºè¿™ä¸ªå€¼ï¼Œåˆ™ä¸è®¤ä¸ºæœ‰æ”¹å–„
        """
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.early_stop = False

    def __call__(self, val_loss):
        if self.best_loss is None:
            self.best_loss = val_loss
        elif val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True


import torch
import torch.nn as nn


class MSEHuberLoss(nn.Module):
    def __init__(self, delta=1.0, alpha=0.5):
        """
        ç»„åˆ MSE å’Œ Huber ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œé€‚ç”¨äº LSTF ä»»åŠ¡
        :param delta: Huber æŸå¤±çš„é˜ˆå€¼
        :param alpha: MSE å’Œ Huber çš„åŠ æƒç³»æ•° (0~1)ï¼Œé»˜è®¤ä¸º 0.5
        """
        super(MSEHuberLoss, self).__init__()
        self.delta = delta
        self.alpha = alpha
        self.mse_loss = nn.MSELoss()

    def huber_loss(self, y_pred, y_true):
        abs_error = torch.abs(y_true - y_pred)
        quadratic = 0.5 * abs_error ** 2
        linear = self.delta * (abs_error - 0.5 * self.delta)
        return torch.where(abs_error < self.delta, quadratic, linear).mean()

    def forward(self, y_pred, y_true):
        mse = self.mse_loss(y_pred, y_true)
        huber = self.huber_loss(y_pred, y_true)
        return self.alpha * mse + (1 - self.alpha) * huber

    def evaluate(self, y_pred, y_true):
        """
        è¯„ä¼°æ¨¡å‹æ—¶è®¡ç®— MSE å’Œ MAE
        :param y_pred: é¢„æµ‹å€¼
        :param y_true: çœŸå®å€¼
        :return: MSE å’Œ MAE
        """
        mse = self.mse_loss(y_pred, y_true)
        mae = self.mae_loss(y_pred, y_true)
        return mse, mae
# class TimeSeriesLoss(nn.Module):
#     """
#     é€‚ç”¨äºæ—¶é—´åºåˆ—æ•°æ®çš„æŸå¤±å‡½æ•°
#     - è®­ç»ƒæ—¶ä½¿ç”¨ MSE ä½œä¸ºä¼˜åŒ–ç›®æ ‡
#     - è¯„ä¼°æ—¶è®¡ç®— MSE å’Œ MAE è¿›è¡Œæ¯”è¾ƒ
#     """
#     def __init__(self):
#         super(TimeSeriesLoss, self).__init__()
#         self.mse_loss = nn.MSELoss()
#         self.mae_loss = nn.L1Loss()
#
#     def forward(self, y_pred, y_true):
#         """
#         :param y_pred: é¢„æµ‹å€¼ (batch_size, output_time_steps, num_features)
#         :param y_true: çœŸå®å€¼ (batch_size, output_time_steps, num_features)
#         :return: MSE æŸå¤±ï¼ˆç”¨äºè®­ç»ƒï¼‰
#         """
#         return self.mse_loss(y_pred, y_true)
#
#     def evaluate(self, y_pred, y_true):
#         """
#         è¯„ä¼°æ¨¡å‹æ—¶è®¡ç®— MSE å’Œ MAE
#         :param y_pred: é¢„æµ‹å€¼
#         :param y_true: çœŸå®å€¼
#         :return: MSE å’Œ MAE
#         """
#         mse = self.mse_loss(y_pred, y_true)
#         mae = self.mae_loss(y_pred, y_true)
#         return mse, mae


def benchmark_inference(model, loader, device):
    """
    å¯¹å‰å‡ ä¸ªæ ·æœ¬è¿›è¡Œæ¨ç†æ—¶é—´ä¸æ˜¾å­˜æµ‹è¯•ã€‚
    """
    model.to(device)
    model.eval()

    inference_times = []
    memory_usages = []

    with torch.no_grad():
        for inputs, targets in loader:
            inputs, targets = inputs.to(device), targets.to(device)
            S, L, D = inputs.shape

            for i in range(S):
                x_single = inputs[i].unsqueeze(0)

                # æ¸…ç†ç¼“å­˜
                torch.cuda.empty_cache()
                torch.cuda.reset_peak_memory_stats()
                torch.cuda.synchronize()

                start_event = torch.cuda.Event(enable_timing=True)
                end_event = torch.cuda.Event(enable_timing=True)

                start_event.record()
                _ = model(x_single)
                end_event.record()

                torch.cuda.synchronize()

                elapsed_time = start_event.elapsed_time(end_event)  # æ¯«ç§’
                peak_memory = torch.cuda.max_memory_allocated() / 1024 / 1024  # MB

                inference_times.append(elapsed_time)
                memory_usages.append(peak_memory)


    # æ‰“å°å¹³å‡æ¨ç†æ—¶é—´å’Œæ˜¾å­˜ä½¿ç”¨
    avg_time = sum(inference_times) / len(inference_times)
    avg_memory = sum(memory_usages) / len(memory_usages)

    print(f"\nâœ… Average Inference Time: {avg_time:.2f} ms")
    print(f"âœ… Average Peak Memory Usage: {avg_memory:.2f} MB")

    return inference_times, memory_usages



def save_training_results(model, optimizer, num_epochs, train_losses, mse, mae, best_samples, model_name, time_step,
                          base_model_dir='./models', base_loss_dir='./loss', base_metric_dir='./metrics',
                          base_samples_dir='./samples'):
    """
    ä»…å½“å½“å‰ MSE ä½äºå†å²æœ€å° MSE æ—¶ï¼Œä¿å­˜è®­ç»ƒæŸå¤±ã€æ¨¡å‹å‚æ•°å’Œæµ‹è¯•æŒ‡æ ‡ã€‚

    :param model: PyTorch è®­ç»ƒæ¨¡å‹
    :param optimizer: ä¼˜åŒ–å™¨
    :param num_epochs: è®­ç»ƒè½®æ•°
    :param train_losses: è®­ç»ƒæŸå¤±åˆ—è¡¨
    :param mse: è¯„ä¼°æŒ‡æ ‡ MSE
    :param mae: è¯„ä¼°æŒ‡æ ‡ MAE
    :param model_name: æ¨¡å‹åç§°
    :param time_step: æ—¶é—´æ­¥é•¿ (å¦‚ 96, 128)
    :param base_model_dir: ä¿å­˜æ¨¡å‹çš„åŸºç¡€è·¯å¾„
    :param base_loss_dir: ä¿å­˜æŸå¤±çš„åŸºç¡€è·¯å¾„
    :param base_metric_dir: ä¿å­˜è¯„ä»·æŒ‡æ ‡çš„åŸºç¡€è·¯å¾„
    """

    # è·å–å½“å‰æ—¶é—´å­—ç¬¦ä¸²ï¼Œæ ¼å¼åŒ–ä¸º 'YYYYMMDD_HHMMSS'ï¼Œç”¨äºç‰ˆæœ¬ç®¡ç†
    current_time = datetime.now().strftime("%Y%m%d_%H%M%S")

    # ================== 1. ç›®å½•ç®¡ç† ==================
    loss_dir = os.path.join(base_loss_dir, 'train_loss')   # è®­ç»ƒæŸå¤±æ–‡ä»¶å¤¹
    model_dir = os.path.join(base_model_dir, 'modelSavePth')  # æ¨¡å‹å‚æ•°æ–‡ä»¶å¤¹
    metric_dir = os.path.join(base_metric_dir, 'metrics')  # è¯„ä»·æŒ‡æ ‡æ–‡ä»¶å¤¹

    # ç¡®ä¿æ‰€æœ‰ç›®å½•å­˜åœ¨
    os.makedirs(loss_dir, exist_ok=True)
    os.makedirs(model_dir, exist_ok=True)
    os.makedirs(metric_dir, exist_ok=True)

    # ================== 2. æ–‡ä»¶è·¯å¾„ ==================
    loss_file_path = os.path.join(loss_dir, f'loss_{model_name}_T{time_step}.csv')  # è®­ç»ƒæŸå¤±æ–‡ä»¶
    metric_file_path = os.path.join(metric_dir, f'metrics_{model_name}_T{time_step}.csv')  # è¯„ä»·æŒ‡æ ‡æ–‡ä»¶
    model_path = os.path.join(model_dir, f'{model_name}_T{time_step}.pth')  # æ¨¡å‹å‚æ•°æ–‡ä»¶

    # ================== 3. è¯»å–å†å²æœ€å° MSE ==================
    best_mse = float('inf')  # åˆå§‹è®¾ä¸ºæ— ç©·å¤§
    if os.path.exists(metric_file_path):
        with open(metric_file_path, 'r') as f:
            reader = csv.reader(f)
            next(reader)  # è·³è¿‡è¡¨å¤´
            for row in reader:
                best_mse = float(row[0])  # è¯»å–å†å²æœ€å° MSE

    # ================== 4. ä»…å½“å½“å‰ MSE ä½äºå†å²æœ€å° MSE æ—¶ï¼Œæ‰ä¿å­˜æ•°æ® ==================
    plot_feature_idx = 5  # exchange_rate = 6ã€ electricity = 5, ETT-small, national_illness, weather=38 , traffic = 5
    if mse < best_mse:
        print(f"ğŸ”¹ å½“å‰ MSE ({mse:.6f}) ä½äºå†å²æœ€å° MSE ({best_mse:.6f})ï¼Œæ›´æ–°æ•°æ®...")

        # ========== (1) ä¿å­˜æ–°çš„è®­ç»ƒæŸå¤± ==========
        with open(loss_file_path, 'w', newline='') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(['Epoch', 'Loss', 'TimeStep'])  # æ·»åŠ æ—¶é—´æ­¥ä¿¡æ¯
            for epoch, loss in enumerate(train_losses, 1):
                writer.writerow([epoch, loss, time_step])

        print(f"âœ… è®­ç»ƒæŸå¤±å·²ä¿å­˜åˆ° {loss_file_path}")

        # ========== (2) ä¿å­˜æ¨¡å‹å‚æ•° ==========
        torch.save({
            'epoch': num_epochs,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': train_losses,
            'mse': mse,  # å­˜å‚¨æœ€ä¼˜ MSE
            'mae': mae,  # å­˜å‚¨æœ€ä¼˜ MAE
            'time_step': time_step,  # é¢å¤–å­˜å‚¨æ—¶é—´æ­¥ä¿¡æ¯
            'timestamp': current_time  # é¢å¤–å­˜å‚¨æ—¶é—´æˆ³
        }, model_path)

        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ° {model_path}")

        # ========== (3) ä¿å­˜æ–°çš„ MSE å’Œ MAE ==========
        with open(metric_file_path, 'w', newline='') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(['MSE', 'MAE', 'TimeStep'])  # ç¡®ä¿ç¬¬ä¸€è¡Œæ˜¯ MSEã€MAE å’Œæ—¶é—´æ­¥é•¿
            writer.writerow([mse, mae, time_step])

        print(f"âœ… è¯„ä»·æŒ‡æ ‡å·²ä¿å­˜åˆ° {metric_file_path}")

        # visualize_best_samples(best_samples, base_samples_dir, plot_feature_idx=plot_feature_idx)

    else:
        # visualize_best_samples(best_samples, base_samples_dir, plot_feature_idx=plot_feature_idx)
        print(f"âŒ å½“å‰ MSE ({mse:.6f}) å¤§äºæˆ–ç­‰äºå†å²æœ€å° MSE ({best_mse:.6f})ï¼Œè·³è¿‡ä¿å­˜ã€‚")


def dataset_selection(datasetSelection):
    """æ ¹æ® datasetSelection é€‰æ‹©å¹¶åŠ è½½ç›¸åº”çš„æ•°æ®é›†"""

    # æ•°æ®é›†è·¯å¾„å­—å…¸
    dataset_dict = {
        "electricity": {
            "dataset_path": os.path.join("..", "six_dataset", "electricity"),
            "vis_path": os.path.join("..", "six_dataset", "electricity", "result", "FDU"),
            "num_features": 321  # è¿™é‡Œæ˜¯ç‰¹å¾ä¸ªæ•°ï¼Œç¤ºä¾‹å€¼
        },
        "ETT-small": {
            "dataset_path": os.path.join("..", "six_dataset", "ETT-small"),
            "vis_path": os.path.join("..", "six_dataset", "ETT-small", "result", "IDS", "S4"),
            "num_features": 7
        },
        "exchange_rate": {
            "dataset_path": os.path.join("..", "six_dataset", "exchange_rate"),
            "vis_path": os.path.join("..", "six_dataset", "exchange_rate", "result", "IDS", "S4"),
            "num_features": 8
        },
        "national_illness": {
            "dataset_path": os.path.join("..", "six_dataset", "illness"),
            "vis_path": os.path.join("..", "six_dataset", "illness", "result", "FDU"),
            "num_features": 7
        },
        "traffic": {
            "dataset_path": os.path.join("..", "six_dataset", "traffic"),
            "vis_path": os.path.join("..", "six_dataset", "traffic", "result", "FDU"),
            "num_features": 862
        },
        "weather": {
            "dataset_path": os.path.join("..", "six_dataset", "weather"),
            "vis_path": os.path.join("..", "six_dataset", "weather", "result", "IDS", "S4"),
            "num_features": 21
        }
    }

    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦æœ‰æ•ˆ
    if datasetSelection not in dataset_dict:
        raise ValueError(f"âŒ æœªçŸ¥çš„æ•°æ®é›†é€‰æ‹©: {datasetSelection}")

    # è·å–æ•°æ®é›†è·¯å¾„
    dataset_info = dataset_dict[datasetSelection]
    dataset_path, vis_path, num_features = dataset_info["dataset_path"], dataset_info["vis_path"], dataset_info["num_features"]

    print(f"âœ… å·²åŠ è½½æ•°æ®é›†: {datasetSelection}")
    return vis_path, dataset_path, num_features  #


import torch
import torch.nn as nn


def validate_model(model, dataloader, device):
    """
    åœ¨æµ‹è¯•é›†ä¸ŠéªŒè¯æ¨¡å‹ï¼Œå¹¶è®¡ç®— MSE å’Œ MAE è¯„ä»·æŒ‡æ ‡ï¼ˆä½¿ç”¨ PyTorch å†…ç½®å‡½æ•°ï¼‰ã€‚

    :param model: è®­ç»ƒå¥½çš„æ·±åº¦å­¦ä¹ æ¨¡å‹
    :param dataloader: æµ‹è¯•æ•°æ®é›†çš„ DataLoader
    :param device: è®¾å¤‡ (CPU or CUDA)
    :return: (MSE, MAE, best_samples)
             best_samples æ˜¯ä¸€ä¸ª dictï¼ŒåŒ…å« 'inputs', 'outputs', 'targets'ï¼Œå¯¹åº” MSE æœ€å°çš„æ‰¹æ¬¡
    """
    model.eval()  # è¿›å…¥è¯„ä¼°æ¨¡å¼
    mse_loss_fn = nn.MSELoss(reduction='mean')  # è®¡ç®—æ€» MSEï¼Œç¨åå½’ä¸€åŒ–
    mae_loss_fn = nn.L1Loss(reduction='mean')  # è®¡ç®—æ€» MAEï¼Œç¨åå½’ä¸€åŒ–

    total_mse, total_mae = 0, 0
    count = len(dataloader)

    best_mse = float('inf')
    best_item = {'inputs': None, 'outputs': None, 'targets': None}

    with torch.no_grad():  # å…³é—­æ¢¯åº¦è®¡ç®—
        for inputs, targets in dataloader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)  # è¿›è¡Œé¢„æµ‹

            batch_mse = mse_loss_fn(outputs, targets).item()
            batch_mae = mae_loss_fn(outputs, targets).item()

            total_mse += batch_mse
            total_mae += batch_mae

            # æ›´æ–°æœ€ä¼˜æ‰¹æ¬¡
            if batch_mse < best_mse:
                best_mse = batch_mse
                # ä¸ºäº†åç»­ä½¿ç”¨ï¼Œä¿å­˜åœ¨ CPU ä¸Šçš„ tensor
                best_item['inputs'] = inputs.cpu()
                best_item['outputs'] = outputs.cpu()
                best_item['targets'] = targets.cpu()

    avg_mse = total_mse / count
    avg_mae = total_mae / count

    return avg_mse / 2, avg_mae / 2, best_item


# **æ·»åŠ  models ç›®å½•åˆ° sys.path**
current_dir = os.path.dirname(os.path.abspath(__file__))  # è·å–å½“å‰ train.py æ‰€åœ¨ç›®å½• (exp)
project_root = os.path.dirname(current_dir)  # è·å– KF-O3S1 ç›®å½•
models_path = os.path.join(project_root, "models")  # ç¡®ä¿ models ç›®å½•åœ¨ sys.path é‡Œ
if models_path not in sys.path:
    sys.path.append(models_path)


def model_selective(modelSelection, num_features):
    """æ ¹æ® modelSelection é€‰æ‹©å¹¶å®ä¾‹åŒ–ä¸åŒçš„æ¨¡å‹"""
    # è®¾å®šè®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # **æ¨¡å‹å­—å…¸ï¼šæ˜ å°„æ¨¡å‹åç§°åˆ° (æ¨¡å—å, å‚æ•°å˜é‡, æ¨¡å‹ç±»)**
    model_dict = {
        "LSTM": ("LSTMDefinite", "paramsLSTM", "LSTMModel"),
        "Mamba": ("MambaDefinite", "paramsMamba", "MambaModel"),
        "Transformer": ("TransformerDefine", "paramsTransformer", "TransformerModel"),
        "KOSS": ("KOSSDefiniteSix", "paramsKOSS", "KOSSModel"),
    }

    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆæ¨¡å‹
    if modelSelection not in model_dict:
        raise ValueError(f"âŒ æœªçŸ¥çš„æ¨¡å‹é€‰æ‹©: {modelSelection}")

    # è·å–æ¨¡å—ã€å‚æ•°ã€æ¨¡å‹åç§°
    module_name, params_name, model_name = model_dict[modelSelection]

    # åŠ¨æ€å¯¼å…¥æ¨¡å—
    module = __import__(module_name, fromlist=[params_name, model_name])

    # è·å–å‚æ•°å’Œæ¨¡å‹
    params = getattr(module, params_name)
    ModelClass = getattr(module, model_name)
    params["input_features"] = num_features
    params["output_features"] = num_features

    # é€šç”¨å‚æ•°è§£æ
    model_kwargs = {
        "input_features": params["input_features"],
        "output_time_steps": params["output_time_steps"],
        "output_features": params["output_features"],
    }

    # å„æ¨¡å‹çš„é¢å¤–å‚æ•°
    if modelSelection == "LSTM":
        model_kwargs.update({
            "lstm_hidden_size": params["lstm_hidden_size"],
            "lstm_num_layers": params["lstm_num_layers"],
            "input_time_steps": params["input_time_steps"],
            "period": params["period"],
        })
    elif modelSelection == "Mamba":
        model_kwargs.update({
            "mamba_hidden_size": params["mamba_hidden_size"],
            "mamba_num_layers": params["mamba_num_layers"],
            "period": params["period"],
        })
    elif modelSelection == "Transformer":
        model_kwargs.update({
            "transformer_hidden_size": params["transformer_hidden_size"],
            "num_transformer_layers": params["num_transformer_layers"],
            "num_heads": params["num_heads"],
            "input_time_steps": params["input_time_steps"],
        })
    elif modelSelection == "KOSS":
        model_kwargs.update({
            "KOSS_hidden_size": params["KOSS_hidden_size"],
            "KOSS_num_layers": params["KOSS_num_layers"],
        })

    # å®ä¾‹åŒ–æ¨¡å‹å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
    model = ModelClass(**model_kwargs).to(device)

    print(f"âœ… å·²é€‰æ‹©æ¨¡å‹: {modelSelection}")
    return params, model, device


def model_searchBest(modelSelection, num_features):
    """æ ¹æ® modelSelection é€‰æ‹©å¹¶å®ä¾‹åŒ–ä¸åŒçš„æ¨¡å‹"""
    # è®¾å®šè®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # **æ¨¡å‹å­—å…¸ï¼šæ˜ å°„æ¨¡å‹åç§°åˆ° (æ¨¡å—å, å‚æ•°å˜é‡, æ¨¡å‹ç±»)**
    model_dict = {
        "LSTM": ("LSTMDefinite", "paramsLSTM", "LSTMModel"),
        "Mamba": ("MambaDefinite", "paramsMamba", "MambaModel"),
        "Transformer": ("TransformerDefine", "paramsTransformer", "TransformerModel"),
        "KOSS": ("KOSSDefiniteSix", "paramsKOSS", "KOSSModel"),
    }

    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆæ¨¡å‹
    if modelSelection not in model_dict:
        raise ValueError(f"âŒ æœªçŸ¥çš„æ¨¡å‹é€‰æ‹©: {modelSelection}")

    # è·å–æ¨¡å—ã€å‚æ•°ã€æ¨¡å‹åç§°
    module_name, params_name, model_name = model_dict[modelSelection]

    # åŠ¨æ€å¯¼å…¥æ¨¡å—
    module = __import__(module_name, fromlist=[params_name, model_name])

    # è·å–å‚æ•°å’Œæ¨¡å‹
    params = getattr(module, params_name)
    ModelClass = getattr(module, model_name)
    params["input_features"] = num_features
    params["output_features"] = num_features

    # é€šç”¨å‚æ•°è§£æ
    model_kwargs = {
        "input_features": params["input_features"],
        "output_time_steps": params["output_time_steps"],
        "output_features": params["output_features"],
    }

    # å„æ¨¡å‹çš„é¢å¤–å‚æ•°
    if modelSelection == "LSTM":
        model_kwargs.update({
            "lstm_hidden_size": params["lstm_hidden_size"],
            "lstm_num_layers": params["lstm_num_layers"],
            "input_time_steps": params["input_time_steps"],
            "period": params["period"],
        })
    elif modelSelection == "Mamba":
        model_kwargs.update({
            "mamba_hidden_size": params["mamba_hidden_size"],
            "mamba_num_layers": params["mamba_num_layers"],
            "period": params["period"],
        })
    elif modelSelection == "Transformer":
        model_kwargs.update({
            "transformer_hidden_size": params["transformer_hidden_size"],
            "num_transformer_layers": params["num_transformer_layers"],
            "num_heads": params["num_heads"],
            "input_time_steps": params["input_time_steps"],
        })
    elif modelSelection == "KOSS":
        model_kwargs.update({
            "KOSS_hidden_size": params["KOSS_hidden_size"],
            "KOSS_num_layers": params["KOSS_num_layers"],
        })

    # # å®ä¾‹åŒ–æ¨¡å‹å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
    # model = ModelClass(**model_kwargs).to(device)

    print(f"âœ… å·²é€‰æ‹©æ¨¡å‹: {modelSelection}")
    return params, ModelClass, device


# é€‰æ‹©æ¨¡å‹
# modelSelection = "LSTM"  # å¯æ›´æ”¹ä¸º "Mamba" æˆ– "Transformer"æˆ–"Mamba"æˆ–"S6KF"
# modelSelection = "Mamba"  # å¯æ›´æ”¹ä¸º "Mamba" æˆ– "Transformer"æˆ–"Mamba"æˆ–"S6KF"
# modelSelection = "Transformer"  # å¯æ›´æ”¹ä¸º "Mamba" æˆ– "Transformer"æˆ–"Mamba"æˆ–"S6KF"
modelSelection = "KOSS"  # å¯æ›´æ”¹ä¸º "Mamba" æˆ– "Transformer"æˆ–"Mamba"æˆ–"S6KF"
datasetSelection = "ETT-small"  # exchange_rateã€ electricity, ETT-small, national_illness, weather, traffic


def main():
    vis_path, dataset_path, num_features = dataset_selection(datasetSelection)

    params, model, device = model_selective(modelSelection, num_features)
    # train_loader, val_loader, test_loader, scaler = load_data(
    #     dataset_path, params["input_time_steps"], params["output_time_steps"], params["batch_size"]
    # )
    train_loader = load_small_inference_set(dataset_path, params["input_time_steps"], params["output_time_steps"], params["batch_size"])
    best_mae = float("inf")
    best_mse = None
    best_train_losses = None
    best_samples = None
    total_mse, total_mae = 0, 0

    for run in range(5):  # è¿›è¡Œ4æ¬¡è®­ç»ƒ
        print(f"\n===== ç¬¬ {run + 1} æ¬¡è®­ç»ƒ =====\n")
        model.apply(lambda m: m.reset_parameters() if hasattr(m, "reset_parameters") else None)
        # è®­ç»ƒ
        inference_times, memory_usages = benchmark_inference(model, train_loader, device)
        count_parameters(model)


if __name__ == "__main__":
    main()
