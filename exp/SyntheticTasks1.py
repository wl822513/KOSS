# åœ¨å¯¼å…¥ Triton æˆ–å…¶ä»–åº“ä¹‹å‰å…ˆåŠ è½½ VS ç¼–è¯‘ç¯å¢ƒ
from exp.selective_copying_mamba_main.init_env import load_visual_studio_env

# åŠ è½½ VS ç¯å¢ƒ
load_visual_studio_env()

import os
import torch
import torch.optim as optim
import logging
import time
from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel
from exp.selective_copying_mamba_main.config import training_config, dataset_config, MambaConfig
from exp.selective_copying_mamba_main.data_generator import generate_dataset, CopyingDataset
from torch.utils.data import DataLoader
import csv
import sys
import torch.nn as nn
import warnings
from datetime import datetime

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
warnings.filterwarnings("ignore")


def save_training_results(model, optimizer, train_losses, Val_Acc, model_name, intensity,
                          base_model_dir='./models', base_loss_dir='./loss', base_metric_dir='./metrics'):
    """
    ä»…å½“å½“å‰ Val_Acc é«˜äºå†å²åŒ intensity æœ€ä¼˜ Val_Acc æ—¶ï¼Œæ›´æ–° summary.csv ä¸­å¯¹åº”è¡Œï¼Œ
    å¹¶ä¿å­˜æœ€æ–°è®­ç»ƒæŸå¤±å’Œæ¨¡å‹å‚æ•°ã€‚

    :param model: PyTorch æ¨¡å‹
    :param optimizer: ä¼˜åŒ–å™¨
    :param train_Acc: éªŒè¯æŒ‡æ ‡ train_Acc
    :param Val_Acc: æµ‹è¯•æŒ‡æ ‡ Val_Acc
    :param model_name: æ¨¡å‹åç§°
    :param intensity: æ±¡æŸ“å¼ºåº¦ï¼ˆå­—ç¬¦ä¸²æˆ–æ•°å­—ï¼‰
    :param base_model_dir: æ¨¡å‹ä¿å­˜æ ¹ç›®å½•
    :param base_loss_dir: è®­ç»ƒæŸå¤±ä¿å­˜æ ¹ç›®å½•
    :param base_metric_dir: è¯„ä»·æŒ‡æ ‡ä¿å­˜æ ¹ç›®å½•
    """

    current_time = datetime.now().strftime("%Y%m%d_%H%M%S")

    # è·¯å¾„è®¾ç½®
    model_dir = os.path.join(base_model_dir, model_name)
    loss_dir = os.path.join(base_loss_dir, model_name)
    metric_dir = os.path.join(base_metric_dir, model_name)
    os.makedirs(model_dir, exist_ok=True)
    os.makedirs(loss_dir, exist_ok=True)
    os.makedirs(metric_dir, exist_ok=True)

    # summary.csv è·¯å¾„
    summary_path = os.path.join(metric_dir, 'summary.csv')

    # è¯»å– summary.csvï¼ŒåŠ è½½æ‰€æœ‰å†å²æŒ‡æ ‡
    summary_data = {}
    if os.path.exists(summary_path):
        try:
            with open(summary_path, 'r', newline='') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    key = row['intensity']
                    summary_data[key] = row
        except Exception as e:
            print(f"âš ï¸ è¯»å– summary.csv å¤±è´¥ï¼Œç»§ç»­é»˜è®¤ç©ºæ•°æ®: {e}")

    intensity_str = str(intensity)

    # æ¯”è¾ƒå½“å‰ Val_Acc ä¸å†å²åŒå¼ºåº¦çš„ Val_Accï¼Œé»˜è®¤0
    best_val_acc = float(summary_data.get(intensity_str, {}).get('Val_Acc', 0.0))

    if Val_Acc > best_val_acc:
        print(f"ğŸ”¹ å½“å‰ Val_Acc ({Val_Acc:.6f}) ä¼˜äºå†å² {best_val_acc:.6f}ï¼Œæ›´æ–°ä¿å­˜æ•°æ®...")

        # æ›´æ–° summary_data
        summary_data[intensity_str] = {
            'Val_Acc': f"{Val_Acc:.6f}",
            'intensity': intensity_str,
            'timestamp': current_time
        }

        # ä¿å­˜ summary.csvï¼ˆæ’åºå†™å…¥ï¼Œæ–¹ä¾¿æŸ¥çœ‹ï¼‰
        with open(summary_path, 'w', newline='') as f:
            fieldnames = ['intensity', 'Val_Acc', 'timestamp']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            # æŒ‰æ•°å€¼å¤§å°æ’åº
            for key in sorted(summary_data.keys(), key=lambda x: float(x)):
                writer.writerow({
                    'intensity': key,
                    'Val_Acc': summary_data[key]['Val_Acc'],
                    'timestamp': summary_data[key].get('timestamp', '')
                })

        print(f"âœ… summary.csv å·²æ›´æ–°: {summary_path}")

        # ä¿å­˜è®­ç»ƒæŸå¤±ï¼ˆè¦†ç›–ï¼‰
        loss_path = os.path.join(loss_dir, f'loss_di{intensity_str}.csv')
        with open(loss_path, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['Epoch', 'Loss', 'intensity'])
            for epoch, loss in enumerate(train_losses, 1):
                writer.writerow([epoch, loss, intensity_str])
        print(f"âœ… è®­ç»ƒæŸå¤±å·²ä¿å­˜: {loss_path}")

        # ä¿å­˜æ¨¡å‹å‚æ•°ï¼ˆè¦†ç›–ï¼‰
        model_path = os.path.join(model_dir, f'model_di{intensity_str}.pth')
        torch.save({
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'train_losses': train_losses,
            'Val_Acc': Val_Acc,
            'intensity': intensity_str,
            'timestamp': current_time
        }, model_path)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {model_path}")

    else:
        print(f"âŒ å½“å‰ Val_Acc ({Val_Acc:.6f}) æœªä¼˜äºå†å² {best_val_acc:.6f}ï¼Œè·³è¿‡ä¿å­˜ã€‚")


# Setup logging
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
# logger = logging.getLogger()

# # Device configuration
# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
# logger.info(f'Using device: {device}')

# Define model
# mambaconfig = MambaConfig()
# model = MambaLMHeadModel(mambaconfig, device=device)
#
# criterion = nn.CrossEntropyLoss()
# optimizer = optim.Adam(model.parameters(), lr=training_config["learning_rate"])

# **æ·»åŠ  models ç›®å½•åˆ° sys.path**
current_dir = os.path.dirname(os.path.abspath(__file__))  # è·å–å½“å‰ train.py æ‰€åœ¨ç›®å½• (exp)
project_root = os.path.dirname(current_dir)  # è·å– KF-O3S1 ç›®å½•
models_path = os.path.join(project_root, "models")  # ç¡®ä¿ models ç›®å½•åœ¨ sys.path é‡Œ
if models_path not in sys.path:
    sys.path.append(models_path)


def model_selective(modelSelection, device):
    """æ ¹æ® modelSelection é€‰æ‹©å¹¶å®ä¾‹åŒ–ä¸åŒçš„æ¨¡å‹"""

    # **æ¨¡å‹å­—å…¸ï¼šæ˜ å°„æ¨¡å‹åç§°åˆ° (æ¨¡å—å, å‚æ•°å˜é‡, æ¨¡å‹ç±»)**
    model_dict = {
        "LSTM": ("LSTMDefinite", "paramsLSTM", "LSTMModel"),
        "Mamba": ("MambaDefiniteSt", "paramsMamba", "MambaModel"),
        "Transformer": ("TransformerDefine", "paramsTransformer", "TransformerModel"),
        "KOSS": ("KOSSDefiniteSt", "paramsKOSS", "KOSSModel"),
    }

    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆæ¨¡å‹
    if modelSelection not in model_dict:
        raise ValueError(f"âŒ æœªçŸ¥çš„æ¨¡å‹é€‰æ‹©: {modelSelection}")

    # è·å–æ¨¡å—ã€å‚æ•°ã€æ¨¡å‹åç§°
    module_name, params_name, model_name = model_dict[modelSelection]

    # åŠ¨æ€å¯¼å…¥æ¨¡å—
    module = __import__(module_name, fromlist=[params_name, model_name])

    # è·å–å‚æ•°å’Œæ¨¡å‹
    params = getattr(module, params_name)
    ModelClass = getattr(module, model_name)

    # é€šç”¨å‚æ•°è§£æ
    model_kwargs = {
        "input_features": params["input_features"],
        "output_features": params["output_features"],
    }

    # å„æ¨¡å‹çš„é¢å¤–å‚æ•°
    if modelSelection == "LSTM":
        model_kwargs.update({
            "lstm_hidden_size": params["lstm_hidden_size"],
            "lstm_num_layers": params["lstm_num_layers"],
            "output_time_steps": params["output_time_steps"],
            "input_time_steps": params["input_time_steps"],
            "period": params["period"],
        })
    elif modelSelection == "Mamba":
        model_kwargs.update({
            "mamba_hidden_size": params["mamba_hidden_size"],
            "mamba_num_layers": params["mamba_num_layers"],
            "output_time_steps": params["output_time_steps"],
            "period": params["period"],
        })
    elif modelSelection == "Transformer":
        model_kwargs.update({
            "transformer_hidden_size": params["transformer_hidden_size"],
            "num_transformer_layers": params["num_transformer_layers"],
            "output_time_steps": params["output_time_steps"],
            "num_heads": params["num_heads"],
            "input_time_steps": params["input_time_steps"],
        })
    elif modelSelection == "KOSS":
        model_kwargs.update({
            "KOSS_hidden_size": params["KOSS_hidden_size"],
            "KOSS_num_layers": params["KOSS_num_layers"],
        })

    # å®ä¾‹åŒ–æ¨¡å‹å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
    model = ModelClass(**model_kwargs).to(device)

    print(f"âœ… å·²é€‰æ‹©æ¨¡å‹: {modelSelection}")
    return model


def setup_logging():
    """
    è®¾ç½®æ—¥å¿—æ ¼å¼å’Œçº§åˆ«ã€‚
    """
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
    return logging.getLogger()


def get_device():
    """
    è·å–å½“å‰ä½¿ç”¨è®¾å¤‡ï¼ˆGPU æˆ– CPUï¼‰ã€‚
    """
    return torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')


def build_model(modelSelection, device):
    """
    åˆå§‹åŒ–æ¨¡å‹ä¸ä¼˜åŒ–å™¨ã€‚
    è¿”å›ï¼šmodel, model_hyperparamsï¼ˆå­—å…¸ï¼Œå¯é€‰ï¼‰
    """
    if modelSelection == "Mamba":
        config = MambaConfig()
        model = MambaLMHeadModel(config, device=device)
    elif modelSelection == "KOSS":
        model = model_selective(modelSelection, device)

    else:
        raise ValueError(f"Unsupported model selection: {modelSelection}")

    return model


def prepare_dataloader():
    """
    æ„å»º DataLoaderï¼Œç”¨äºè®­ç»ƒã€‚
    """
    train_dataset = CopyingDataset(dataset_config, training_config)
    num_workers = min(8, os.cpu_count())
    train_loader = DataLoader(
        train_dataset,
        batch_size=1,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True
    )
    return train_loader


def train_one_page(model, optimizer, criterion, device, logger, Page):
    """
    è®­ç»ƒä¸€ä¸ª pageï¼ˆç›¸å½“äºä¸€ä¸ª epochï¼‰ï¼ŒåŒ…å«å¤šä¸ª stepï¼ˆæ¯ step æ˜¯ä¸€ä¸ª batchï¼‰ã€‚
    """
    model.train()
    train_dataset = CopyingDataset(dataset_config, training_config)
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=False)  # batch_size=1 æ˜¯å› ä¸º __getitem__ è¿”å›çš„æ˜¯ (batch_size, seq_len)
    steps_per_page = len(train_dataset)

    for step, (inputs, targets) in enumerate(train_loader):
        # inputs, targets: (1, batch_size, seq_len or l_memorize)
        inputs = inputs.squeeze(0).to(device)   # shape: (batch_size, seq_len)
        targets = targets.squeeze(0).to(device)  # shape: (batch_size, l_memorize)

        outputs = model(inputs, num_last_tokens=dataset_config['l_memorize']).logits
        loss = criterion(outputs, targets)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        with torch.no_grad():
            correct = (outputs.argmax(1) == targets).sum().item()
            total = targets.numel()
            accuracy = 100 * correct / total
            progress = 100 * (Page + 1) / training_config["num_page"]

        logger.info(
            f'Page: {Page + 1}/{training_config["num_page"]} ({progress:.2f}%) | '
            f'Step [{step+1}/{steps_per_page}] | '
            f'Loss: {loss.item():.4f} | '
            f'Accuracy: {accuracy:.2f}%'
        )
    return loss.item()


# def train(model, optimizer, criterion, device, logger):
#     """
#     æ‰§è¡Œæ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ã€‚
#     """
#     logger.info(f"Training on device: {device}")
#     start_time = time.time()
#     total = 0
#
#     for page in range(training_config["num_page"]):
#         logger.info(f"--- Page {page + 1}/{training_config['num_page']} ---")
#         total += train_one_page(model, optimizer, criterion, device, logger, page)
#
#     end_time = time.time()
#     logger.info(f"Training completed in {(end_time - start_time) / 60:.2f} minutes.")
#     return total/training_config["num_page"]
def train(model, optimizer, criterion, device, logger):
    """
    æ‰§è¡Œæ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ã€‚
    """
    model.train()
    logger.info(f"Training on device: {device}")
    start_time = time.time()
    train_losses = []  # ç”¨äºå­˜å‚¨æ¯æ¬¡è®­ç»ƒçš„æŸå¤±å€¼

    for page in range(training_config["num_page"]):
        logger.info(f"--- Page {page + 1}/{training_config['num_page']} ---")
        loss = train_one_page(model, optimizer, criterion, device, logger, page)
        train_losses.append(loss)  # å°†æ¯æ¬¡çš„æŸå¤±å€¼æ·»åŠ åˆ°åˆ—è¡¨ä¸­
        logger.info(f"Page {page + 1} loss: {loss:.4f}")

    end_time = time.time()
    logger.info(f"Training completed in {(end_time - start_time) / 60:.2f} minutes.")
    return train_losses  # è¿”å›æŸå¤±åˆ—è¡¨


def validate(model, device, logger):
    """
    éªŒè¯æ¨¡å‹æ€§èƒ½ã€‚
    """
    model.eval()
    with torch.no_grad():
        inputs, targets = generate_dataset(dataset_config, training_config)
        inputs = inputs.to(device)
        targets = targets.to(device)

        outputs = model(inputs, num_last_tokens=dataset_config['l_memorize']).logits
        correct = (outputs.argmax(1) == targets).sum().item()
        total = targets.size(0) * targets.size(1)
        accuracy = 100 * correct / total
        logger.info(f"Validation Accuracy: {accuracy:.2f}%")
    return accuracy


# é€‰æ‹©æ¨¡å‹
# modelSelection = "LSTM"  # å¯æ›´æ”¹ä¸º "Mamba" æˆ– "Transformer"æˆ–"Mamba"æˆ–"S6KF"
modelSelection = "Mamba"  # å¯æ›´æ”¹ä¸º "Mamba" æˆ– "Transformer"æˆ–"Mamba"æˆ–"S6KF"
# modelSelection = "Transformer"  # å¯æ›´æ”¹ä¸º "Mamba" æˆ– "Transformer"æˆ–"Mamba"æˆ–"S6KF"
# modelSelection = "KOSS"  # å¯æ›´æ”¹ä¸º "Mamba" æˆ– "Transformer"æˆ–"Mamba"æˆ–"S6KF"


def main():
    """
    ä¸»å‡½æ•°ï¼šåˆå§‹åŒ–ç»„ä»¶å¹¶æ‰§è¡Œè®­ç»ƒä¸éªŒè¯ã€‚
    """
    logger = setup_logging()
    device = get_device()
    model = build_model(modelSelection, device)
    model.to(device)

    optimizer = optim.Adam(model.parameters(), lr=training_config["learning_rate"])
    criterion = nn.CrossEntropyLoss()

    loss = train(model, optimizer, criterion, device, logger)
    accuracy = validate(model, device, logger)

    base_model_dir = './return/models'
    base_loss_dir = './return/loss'
    base_metric_dir = './return/metrics'
    save_training_results(model, optimizer, loss, accuracy, modelSelection, dataset_config["intensity"],
                          base_model_dir, base_loss_dir, base_metric_dir)


if __name__ == '__main__':
    main()




