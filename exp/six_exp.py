"""
æ—¥å¿—ï¼š
20240530: ç”¨æ ‡å‡†çš„å»ºæ¨¡æ¶æ„
20240530ï¼šè¦å…ˆå°†ç»çº¬åº¦è¿™ç§ç»å¯¹æ•°æ®è½¬æ¢ä¸ºdis-angè¿™ç§ç›¸å¯¹æ•°æ®ï¼Œä½†è®­ç»ƒè¦ç”¨ç»çº¬åº¦æ•°æ®é‡Œé¿å…0-360Â°ç¿»è½¬çš„é—®é¢˜
20240531: å¼•å…¥åŠ æ€§æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ•ˆæœéå¸¸å¥½
20240606: LSTM+transformer+KF+è‡ªæ³¨æ„åŠ› æ•ˆæœéå¸¸å¥½
20240615: æ”¹æˆå›ºå®šå‘¨æœŸ,é‡å†™lstm_model_test
20240615: æ”¹å†™äº†å„ç§æ³¨æ„åŠ›æ¨¡å‹
20240615: æ”¹å†™æˆinformer(æ•ˆæœå¾ˆå¥½)
20240618: testé‡Œèåˆé¢„æµ‹å€¼å’Œè§‚æµ‹å€¼
20240618: testé‡Œèåˆé¢„æµ‹å€¼å’Œè§‚æµ‹å€¼diså’Œangåˆ†å¼€èåˆ
"""
import torch
import numpy as np
import csv
import sys
import torch.nn as nn
import time
from tqdm import tqdm
from torch.utils.data import TensorDataset, DataLoader
from functions import inverse_standardize, cartesian_to_polar, haversine_distances
from matplot_func import polar_plot_actual_pred1, plot_training_losses
# ç°åœ¨å¯ä»¥å¯¼å…¥ src é‡Œçš„æ¨¡å—
from dataloaders.prepare.six.six_datasets_loader import load_data
from sklearn.metrics import mean_squared_error, mean_absolute_error
import csv
from datetime import datetime  # ç”¨äºè·å–å½“å‰æ—¶é—´


import pandas as pd
import os
import warnings

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
warnings.filterwarnings("ignore")
import matplotlib.pyplot as plt
from datetime import datetime


import os
import csv
import numpy as np
import matplotlib.pyplot as plt

def visualize_best_samples(best_samples, base_samples_dir='./samples', plot_feature_idx=6):
    inputs = best_samples['inputs']   # (batch_size, seq_len_in, feature_dim)
    outputs = best_samples['outputs']  # (batch_size, seq_len_out, feature_dim)
    targets = best_samples['targets']  # (batch_size, seq_len_out, feature_dim)

    input_seq = inputs[0].cpu().numpy()   # (seq_len_in, feature_dim)
    target_seq = targets[0].cpu().numpy() # (seq_len_out, feature_dim)
    output_seq = outputs[0].cpu().numpy() # (seq_len_out, feature_dim)

    seq_len_in, feature_dim = input_seq.shape
    seq_len_out = target_seq.shape[0]

    # ================== å¯è§†åŒ–ï¼ˆåªç”»ä¸€ä¸ªç‰¹å¾ï¼‰ ==================
    real_seq_plot = np.concatenate([input_seq[:, plot_feature_idx], target_seq[:, plot_feature_idx]])
    pred_seq_plot = np.concatenate([np.full(seq_len_in, np.nan), output_seq[:, plot_feature_idx]])

    plt.figure(figsize=(12, 6))
    plt.plot(real_seq_plot, label='Real (Input + Target)', color='blue')
    plt.plot(pred_seq_plot, label='Prediction (Output)', color='orange')
    plt.legend()
    plt.title(f'Best batch example: Feature {plot_feature_idx}')
    plt.xlabel('Time step')
    plt.ylabel('Value')
    plt.tight_layout()
    plt.show()

    # ================== ä¿å­˜ä¸º CSVï¼ˆä¿å­˜æ‰€æœ‰ç‰¹å¾ï¼‰ ==================
    os.makedirs(base_samples_dir, exist_ok=True)
    csv_path = os.path.join(base_samples_dir, f'best_sequence_{inputs.shape[1]}.csv')

    with open(csv_path, 'w', newline='') as f:
        writer = csv.writer(f)
        # å†™è¡¨å¤´
        header = ['TimeStep']
        for i in range(feature_dim):
            header += [f'real_f{i}', f'pred_f{i}']
        writer.writerow(header)

        # å†™æ•°æ®è¡Œ
        for t in range(seq_len_in + seq_len_out):
            row = [t]
            for i in range(feature_dim):
                # çœŸå®å€¼
                if t < seq_len_in:
                    real_val = input_seq[t, i]
                else:
                    real_val = target_seq[t - seq_len_in, i]
                # é¢„æµ‹å€¼
                if t < seq_len_in:
                    pred_val = np.nan
                else:
                    pred_val = output_seq[t - seq_len_in, i]
                row += [real_val, pred_val]
            writer.writerow(row)

    print(f"âœ… æ‰€æœ‰ç‰¹å¾çš„æ—¶é—´åºåˆ—å·²ä¿å­˜ä¸º CSV: {csv_path}")




class EarlyStopping:
    def __init__(self, patience=5, min_delta=0.001):
        """
        æ—©åœç±»ï¼Œç”¨äºç›‘æ§éªŒè¯æŸå¤±
        Args:
            patience (int): æ²¡æœ‰æ”¹å–„çš„éªŒè¯æŸå¤±å…è®¸çš„æœ€å¤§æ¬¡æ•°
            min_delta (float): æœ€å°æ”¹å–„å€¼ï¼Œå¦‚æœå˜åŒ–å°äºè¿™ä¸ªå€¼ï¼Œåˆ™ä¸è®¤ä¸ºæœ‰æ”¹å–„
        """
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.early_stop = False

    def __call__(self, val_loss):
        if self.best_loss is None:
            self.best_loss = val_loss
        elif val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True


import torch
import torch.nn as nn


class MSEHuberLoss(nn.Module):
    def __init__(self, delta=1.0, alpha=0.5):
        """
        ç»„åˆ MSE å’Œ Huber ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œé€‚ç”¨äº LSTF ä»»åŠ¡
        :param delta: Huber æŸå¤±çš„é˜ˆå€¼
        :param alpha: MSE å’Œ Huber çš„åŠ æƒç³»æ•° (0~1)ï¼Œé»˜è®¤ä¸º 0.5
        """
        super(MSEHuberLoss, self).__init__()
        self.delta = delta
        self.alpha = alpha
        self.mse_loss = nn.MSELoss()

    def huber_loss(self, y_pred, y_true):
        abs_error = torch.abs(y_true - y_pred)
        quadratic = 0.5 * abs_error ** 2
        linear = self.delta * (abs_error - 0.5 * self.delta)
        return torch.where(abs_error < self.delta, quadratic, linear).mean()

    def forward(self, y_pred, y_true):
        mse = self.mse_loss(y_pred, y_true)
        huber = self.huber_loss(y_pred, y_true)
        return self.alpha * mse + (1 - self.alpha) * huber

    def evaluate(self, y_pred, y_true):
        """
        è¯„ä¼°æ¨¡å‹æ—¶è®¡ç®— MSE å’Œ MAE
        :param y_pred: é¢„æµ‹å€¼
        :param y_true: çœŸå®å€¼
        :return: MSE å’Œ MAE
        """
        mse = self.mse_loss(y_pred, y_true)
        mae = self.mae_loss(y_pred, y_true)
        return mse, mae
# class TimeSeriesLoss(nn.Module):
#     """
#     é€‚ç”¨äºæ—¶é—´åºåˆ—æ•°æ®çš„æŸå¤±å‡½æ•°
#     - è®­ç»ƒæ—¶ä½¿ç”¨ MSE ä½œä¸ºä¼˜åŒ–ç›®æ ‡
#     - è¯„ä¼°æ—¶è®¡ç®— MSE å’Œ MAE è¿›è¡Œæ¯”è¾ƒ
#     """
#     def __init__(self):
#         super(TimeSeriesLoss, self).__init__()
#         self.mse_loss = nn.MSELoss()
#         self.mae_loss = nn.L1Loss()
#
#     def forward(self, y_pred, y_true):
#         """
#         :param y_pred: é¢„æµ‹å€¼ (batch_size, output_time_steps, num_features)
#         :param y_true: çœŸå®å€¼ (batch_size, output_time_steps, num_features)
#         :return: MSE æŸå¤±ï¼ˆç”¨äºè®­ç»ƒï¼‰
#         """
#         return self.mse_loss(y_pred, y_true)
#
#     def evaluate(self, y_pred, y_true):
#         """
#         è¯„ä¼°æ¨¡å‹æ—¶è®¡ç®— MSE å’Œ MAE
#         :param y_pred: é¢„æµ‹å€¼
#         :param y_true: çœŸå®å€¼
#         :return: MSE å’Œ MAE
#         """
#         mse = self.mse_loss(y_pred, y_true)
#         mae = self.mae_loss(y_pred, y_true)
#         return mse, mae


def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device, min_lr=0.0003, patience=1):
    """
    è®­ç»ƒæ¨¡å‹å¹¶æ·»åŠ éªŒè¯é€»è¾‘ã€‚

    :param model: è®­ç»ƒçš„ PyTorch æ¨¡å‹
    :param train_loader: è®­ç»ƒæ•°æ®é›†çš„ DataLoader
    :param val_loader: éªŒè¯æ•°æ®é›†çš„ DataLoader
    :param criterion: æŸå¤±å‡½æ•°
    :param optimizer: ä¼˜åŒ–å™¨
    :param scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨
    :param input_features: è¾“å…¥ç‰¹å¾æ•°
    :param output_features: è¾“å‡ºç‰¹å¾æ•°
    :param num_epochs: è®­ç»ƒè½®æ•°
    :param device: è®­ç»ƒè®¾å¤‡ (CPU / GPU)
    :param min_lr: å­¦ä¹ ç‡çš„æœ€å°å€¼
    :param patience: æ—©åœçš„è€å¿ƒå€¼ï¼ˆå¤šå°‘ä¸ª epoch éªŒè¯æŸå¤±æœªæå‡åˆ™åœæ­¢ï¼‰
    :return: è®­ç»ƒæŸå¤±ã€éªŒè¯æŸå¤±
    """

    early_stopping = EarlyStopping(patience=patience, min_delta=min_lr)
    train_losses, val_losses = [], []

    model.to(device)
    count_parameters(model)  # âœ… æ”¾åœ¨è¿™é‡Œåªç»Ÿè®¡/æ‰“å°ä¸€æ¬¡

    for epoch in range(num_epochs):
        torch.cuda.reset_peak_memory_stats(device)  # âœ… æ’åœ¨æ¨¡å‹é€å…¥è®¾å¤‡ä¹‹å
        # ====== è®­ç»ƒé˜¶æ®µ ======
        model.train()
        epoch_train_loss = 0
        pbar = tqdm(train_loader, desc=f"Epoch [{epoch + 1}/{num_epochs}]")

        for inputs, targets in pbar:
            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            epoch_train_loss += loss.item()
            pbar.set_postfix({'Train Loss': f'{loss.item():.6e}'})

        peak_memory = torch.cuda.max_memory_allocated(device) / (1024 ** 2)
        print(f"ğŸš€ Peak GPU memory usage (epoch {epoch + 1}): {peak_memory:.2f} MB")
        avg_train_loss = epoch_train_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        pbar.close()

        # ====== éªŒè¯é˜¶æ®µ ======
        model.eval()
        epoch_val_loss = 0

        with torch.no_grad():
            for inputs, targets in val_loader:
                inputs, targets = inputs.to(device), targets.to(device)
                outputs = model(inputs)
                val_loss = criterion(outputs, targets)
                epoch_val_loss += val_loss.item()

        avg_val_loss = epoch_val_loss / len(val_loader)
        val_losses.append(avg_val_loss)

        print(f"Epoch [{epoch + 1}/{num_epochs}] | Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f}")

        # å­¦ä¹ ç‡è°ƒåº¦å™¨æ›´æ–°
        scheduler.step(avg_val_loss)  # è¿™é‡Œä½¿ç”¨éªŒè¯æŸå¤±è°ƒæ•´å­¦ä¹ ç‡
        for param_group in optimizer.param_groups:
            if param_group['lr'] < min_lr:
                param_group['lr'] = min_lr

        # æ£€æŸ¥æ—©åœï¼ˆåŸºäºéªŒè¯æŸå¤±ï¼‰
        early_stopping(avg_val_loss)
        if early_stopping.early_stop:
            print(f"â¹ï¸ Early stopping at epoch {epoch + 1} due to no improvement in validation loss.")
            break

    return train_losses, val_losses


def save_training_results(model, optimizer, num_epochs, train_losses, mse, mae, best_samples, model_name, time_step,
                          base_model_dir='./models', base_loss_dir='./loss', base_metric_dir='./metrics',
                          base_samples_dir='./samples'):
    """
    ä»…å½“å½“å‰ MSE ä½äºå†å²æœ€å° MSE æ—¶ï¼Œä¿å­˜è®­ç»ƒæŸå¤±ã€æ¨¡å‹å‚æ•°å’Œæµ‹è¯•æŒ‡æ ‡ã€‚

    :param model: PyTorch è®­ç»ƒæ¨¡å‹
    :param optimizer: ä¼˜åŒ–å™¨
    :param num_epochs: è®­ç»ƒè½®æ•°
    :param train_losses: è®­ç»ƒæŸå¤±åˆ—è¡¨
    :param mse: è¯„ä¼°æŒ‡æ ‡ MSE
    :param mae: è¯„ä¼°æŒ‡æ ‡ MAE
    :param model_name: æ¨¡å‹åç§°
    :param time_step: æ—¶é—´æ­¥é•¿ (å¦‚ 96, 128)
    :param base_model_dir: ä¿å­˜æ¨¡å‹çš„åŸºç¡€è·¯å¾„
    :param base_loss_dir: ä¿å­˜æŸå¤±çš„åŸºç¡€è·¯å¾„
    :param base_metric_dir: ä¿å­˜è¯„ä»·æŒ‡æ ‡çš„åŸºç¡€è·¯å¾„
    """

    # è·å–å½“å‰æ—¶é—´å­—ç¬¦ä¸²ï¼Œæ ¼å¼åŒ–ä¸º 'YYYYMMDD_HHMMSS'ï¼Œç”¨äºç‰ˆæœ¬ç®¡ç†
    current_time = datetime.now().strftime("%Y%m%d_%H%M%S")

    # ================== 1. ç›®å½•ç®¡ç† ==================
    loss_dir = os.path.join(base_loss_dir, 'train_loss')   # è®­ç»ƒæŸå¤±æ–‡ä»¶å¤¹
    model_dir = os.path.join(base_model_dir, 'modelSavePth')  # æ¨¡å‹å‚æ•°æ–‡ä»¶å¤¹
    metric_dir = os.path.join(base_metric_dir, 'metrics')  # è¯„ä»·æŒ‡æ ‡æ–‡ä»¶å¤¹

    # ç¡®ä¿æ‰€æœ‰ç›®å½•å­˜åœ¨
    os.makedirs(loss_dir, exist_ok=True)
    os.makedirs(model_dir, exist_ok=True)
    os.makedirs(metric_dir, exist_ok=True)

    # ================== 2. æ–‡ä»¶è·¯å¾„ ==================
    loss_file_path = os.path.join(loss_dir, f'loss_{model_name}_T{time_step}.csv')  # è®­ç»ƒæŸå¤±æ–‡ä»¶
    metric_file_path = os.path.join(metric_dir, f'metrics_{model_name}_T{time_step}.csv')  # è¯„ä»·æŒ‡æ ‡æ–‡ä»¶
    model_path = os.path.join(model_dir, f'{model_name}_T{time_step}.pth')  # æ¨¡å‹å‚æ•°æ–‡ä»¶

    # ================== 3. è¯»å–å†å²æœ€å° MSE ==================
    best_mse = float('inf')  # åˆå§‹è®¾ä¸ºæ— ç©·å¤§
    if os.path.exists(metric_file_path):
        with open(metric_file_path, 'r') as f:
            reader = csv.reader(f)
            next(reader)  # è·³è¿‡è¡¨å¤´
            for row in reader:
                best_mse = float(row[0])  # è¯»å–å†å²æœ€å° MSE

    # ================== 4. ä»…å½“å½“å‰ MSE ä½äºå†å²æœ€å° MSE æ—¶ï¼Œæ‰ä¿å­˜æ•°æ® ==================
    plot_feature_idx = 5  # exchange_rate = 6ã€ electricity = 5, ETT-small, national_illness, weather=38 , traffic = 5
    if mse < best_mse:
        print(f"ğŸ”¹ å½“å‰ MSE ({mse:.6f}) ä½äºå†å²æœ€å° MSE ({best_mse:.6f})ï¼Œæ›´æ–°æ•°æ®...")

        # ========== (1) ä¿å­˜æ–°çš„è®­ç»ƒæŸå¤± ==========
        with open(loss_file_path, 'w', newline='') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(['Epoch', 'Loss', 'TimeStep'])  # æ·»åŠ æ—¶é—´æ­¥ä¿¡æ¯
            for epoch, loss in enumerate(train_losses, 1):
                writer.writerow([epoch, loss, time_step])

        print(f"âœ… è®­ç»ƒæŸå¤±å·²ä¿å­˜åˆ° {loss_file_path}")

        # ========== (2) ä¿å­˜æ¨¡å‹å‚æ•° ==========
        torch.save({
            'epoch': num_epochs,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': train_losses,
            'mse': mse,  # å­˜å‚¨æœ€ä¼˜ MSE
            'mae': mae,  # å­˜å‚¨æœ€ä¼˜ MAE
            'time_step': time_step,  # é¢å¤–å­˜å‚¨æ—¶é—´æ­¥ä¿¡æ¯
            'timestamp': current_time  # é¢å¤–å­˜å‚¨æ—¶é—´æˆ³
        }, model_path)

        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ° {model_path}")

        # ========== (3) ä¿å­˜æ–°çš„ MSE å’Œ MAE ==========
        with open(metric_file_path, 'w', newline='') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(['MSE', 'MAE', 'TimeStep'])  # ç¡®ä¿ç¬¬ä¸€è¡Œæ˜¯ MSEã€MAE å’Œæ—¶é—´æ­¥é•¿
            writer.writerow([mse, mae, time_step])

        print(f"âœ… è¯„ä»·æŒ‡æ ‡å·²ä¿å­˜åˆ° {metric_file_path}")

        # visualize_best_samples(best_samples, base_samples_dir, plot_feature_idx=plot_feature_idx)

    else:
        # visualize_best_samples(best_samples, base_samples_dir, plot_feature_idx=plot_feature_idx)
        print(f"âŒ å½“å‰ MSE ({mse:.6f}) å¤§äºæˆ–ç­‰äºå†å²æœ€å° MSE ({best_mse:.6f})ï¼Œè·³è¿‡ä¿å­˜ã€‚")


def dataset_selection(datasetSelection):
    """æ ¹æ® datasetSelection é€‰æ‹©å¹¶åŠ è½½ç›¸åº”çš„æ•°æ®é›†"""

    # æ•°æ®é›†è·¯å¾„å­—å…¸
    dataset_dict = {
        "electricity": {
            "dataset_path": os.path.join("..", "six_dataset", "electricity"),
            "vis_path": os.path.join("..", "six_dataset", "electricity", "result", "FDU"),
            "num_features": 321  # è¿™é‡Œæ˜¯ç‰¹å¾ä¸ªæ•°ï¼Œç¤ºä¾‹å€¼
        },
        "ETT-small": {
            "dataset_path": os.path.join("..", "six_dataset", "ETT-small"),
            "vis_path": os.path.join("..", "six_dataset", "ETT-small", "result", "IDS", "Attention"),
            "num_features": 7
        },
        "exchange_rate": {
            "dataset_path": os.path.join("..", "six_dataset", "exchange_rate"),
            "vis_path": os.path.join("..", "six_dataset", "exchange_rate", "result", "IDS", "S4"),
            "num_features": 8
        },
        "national_illness": {
            "dataset_path": os.path.join("..", "six_dataset", "illness"),
            "vis_path": os.path.join("..", "six_dataset", "illness", "result", "FDU"),
            "num_features": 7
        },
        "traffic": {
            "dataset_path": os.path.join("..", "six_dataset", "traffic"),
            "vis_path": os.path.join("..", "six_dataset", "traffic", "result", "FDU"),
            "num_features": 862
        },
        "weather": {
            "dataset_path": os.path.join("..", "six_dataset", "weather"),
            "vis_path": os.path.join("..", "six_dataset", "weather", "result", "FDU", "LSTM"),
            "num_features": 21
        }
    }

    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦æœ‰æ•ˆ
    if datasetSelection not in dataset_dict:
        raise ValueError(f"âŒ æœªçŸ¥çš„æ•°æ®é›†é€‰æ‹©: {datasetSelection}")

    # è·å–æ•°æ®é›†è·¯å¾„
    dataset_info = dataset_dict[datasetSelection]
    dataset_path, vis_path, num_features = dataset_info["dataset_path"], dataset_info["vis_path"], dataset_info["num_features"]

    print(f"âœ… å·²åŠ è½½æ•°æ®é›†: {datasetSelection}")
    return vis_path, dataset_path, num_features  #


import torch
import torch.nn as nn


def validate_model(model, dataloader, device):
    """
    åœ¨æµ‹è¯•é›†ä¸ŠéªŒè¯æ¨¡å‹ï¼Œå¹¶è®¡ç®— MSE å’Œ MAE è¯„ä»·æŒ‡æ ‡ï¼ˆä½¿ç”¨ PyTorch å†…ç½®å‡½æ•°ï¼‰ã€‚

    :param model: è®­ç»ƒå¥½çš„æ·±åº¦å­¦ä¹ æ¨¡å‹
    :param dataloader: æµ‹è¯•æ•°æ®é›†çš„ DataLoader
    :param device: è®¾å¤‡ (CPU or CUDA)
    :return: (MSE, MAE, best_samples)
             best_samples æ˜¯ä¸€ä¸ª dictï¼ŒåŒ…å« 'inputs', 'outputs', 'targets'ï¼Œå¯¹åº” MSE æœ€å°çš„æ‰¹æ¬¡
    """
    model.eval()  # è¿›å…¥è¯„ä¼°æ¨¡å¼
    mse_loss_fn = nn.MSELoss(reduction='mean')  # è®¡ç®—æ€» MSEï¼Œç¨åå½’ä¸€åŒ–
    mae_loss_fn = nn.L1Loss(reduction='mean')  # è®¡ç®—æ€» MAEï¼Œç¨åå½’ä¸€åŒ–

    total_mse, total_mae = 0, 0
    count = len(dataloader)

    best_mse = float('inf')
    best_item = {'inputs': None, 'outputs': None, 'targets': None}

    with torch.no_grad():  # å…³é—­æ¢¯åº¦è®¡ç®—
        for inputs, targets in dataloader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)  # è¿›è¡Œé¢„æµ‹

            batch_mse = mse_loss_fn(outputs, targets).item()
            batch_mae = mae_loss_fn(outputs, targets).item()

            total_mse += batch_mse
            total_mae += batch_mae

            # æ›´æ–°æœ€ä¼˜æ‰¹æ¬¡
            if batch_mse < best_mse:
                best_mse = batch_mse
                # ä¸ºäº†åç»­ä½¿ç”¨ï¼Œä¿å­˜åœ¨ CPU ä¸Šçš„ tensor
                best_item['inputs'] = inputs.cpu()
                best_item['outputs'] = outputs.cpu()
                best_item['targets'] = targets.cpu()

    avg_mse = total_mse / count
    avg_mae = total_mae / count

    return avg_mse, avg_mae, best_item


def benchmark_throughput(model, dataloader, device='cuda', warmup=10, trials=100):
    """
    ç”¨æµ‹è¯•é›†ä¸­çš„çœŸå®æ•°æ®æµ‹é‡æ¨¡å‹æ¨ç†ååé‡ï¼ˆsamples/sï¼‰

    å‚æ•°:
        model: å·²åŠ è½½å‚æ•°çš„æ¨¡å‹
        dataloader: ç”¨äºæ¨ç†çš„ DataLoaderï¼ˆå»ºè®®ç”¨ test_loaderï¼‰
        device: è®¾å¤‡ (é»˜è®¤ 'cuda')
        warmup: é¢„çƒ­æ¬¡æ•°ï¼ˆé»˜è®¤ 10ï¼‰
        trials: æ­£å¼æµ‹é‡çš„æ‰¹æ¬¡æ•°ï¼ˆé»˜è®¤ 100ï¼‰
    """
    model.to(device)
    model.eval()

    it = iter(dataloader)
    timings = []

    with torch.no_grad():
        # é¢„çƒ­é˜¶æ®µï¼ˆä¸è®¡æ—¶ï¼‰
        for _ in range(warmup):
            try:
                inputs, _ = next(it)
            except StopIteration:
                it = iter(dataloader)
                inputs, _ = next(it)
            inputs = inputs.to(device)
            _ = model(inputs)

        torch.cuda.synchronize()

        # æ­£å¼è®¡æ—¶
        for _ in range(trials):
            try:
                inputs, _ = next(it)
            except StopIteration:
                it = iter(dataloader)
                inputs, _ = next(it)
            inputs = inputs.to(device)

            torch.cuda.synchronize()
            start = time.time()
            _ = model(inputs)
            torch.cuda.synchronize()
            end = time.time()

            timings.append(end - start)

    avg_latency = sum(timings) / len(timings)
    batch_size = inputs.size(0)
    throughput = batch_size / avg_latency

    print(f"ğŸ“ˆ Avg Latency per batch: {avg_latency * 1000:.2f} ms")
    print(f"âš¡ Throughput: {throughput:.2f} samples/s")
    return avg_latency, throughput


# **æ·»åŠ  models ç›®å½•åˆ° sys.path**
current_dir = os.path.dirname(os.path.abspath(__file__))  # è·å–å½“å‰ train.py æ‰€åœ¨ç›®å½• (exp)
project_root = os.path.dirname(current_dir)  # è·å– KF-O3S1 ç›®å½•
models_path = os.path.join(project_root, "models")  # ç¡®ä¿ models ç›®å½•åœ¨ sys.path é‡Œ
if models_path not in sys.path:
    sys.path.append(models_path)


def model_selective(modelSelection, num_features):
    """æ ¹æ® modelSelection é€‰æ‹©å¹¶å®ä¾‹åŒ–ä¸åŒçš„æ¨¡å‹"""
    # è®¾å®šè®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # **æ¨¡å‹å­—å…¸ï¼šæ˜ å°„æ¨¡å‹åç§°åˆ° (æ¨¡å—å, å‚æ•°å˜é‡, æ¨¡å‹ç±»)**
    model_dict = {
        "LSTM": ("LSTMDefinite", "paramsLSTM", "LSTMModel"),
        "Mamba": ("MambaDefinite", "paramsMamba", "MambaModel"),
        "Transformer": ("TransformerDefine", "paramsTransformer", "TransformerModel"),
        "KOSS": ("KOSSDefiniteSix", "paramsKOSS", "KOSSModel"),
    }

    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆæ¨¡å‹
    if modelSelection not in model_dict:
        raise ValueError(f"âŒ æœªçŸ¥çš„æ¨¡å‹é€‰æ‹©: {modelSelection}")

    # è·å–æ¨¡å—ã€å‚æ•°ã€æ¨¡å‹åç§°
    module_name, params_name, model_name = model_dict[modelSelection]

    # åŠ¨æ€å¯¼å…¥æ¨¡å—
    module = __import__(module_name, fromlist=[params_name, model_name])

    # è·å–å‚æ•°å’Œæ¨¡å‹
    params = getattr(module, params_name)
    ModelClass = getattr(module, model_name)
    params["input_features"] = num_features
    params["output_features"] = num_features

    # é€šç”¨å‚æ•°è§£æ
    model_kwargs = {
        "input_features": params["input_features"],
        "output_time_steps": params["output_time_steps"],
        "output_features": params["output_features"],
    }

    # å„æ¨¡å‹çš„é¢å¤–å‚æ•°
    if modelSelection == "LSTM":
        model_kwargs.update({
            "lstm_hidden_size": params["lstm_hidden_size"],
            "lstm_num_layers": params["lstm_num_layers"],
            "input_time_steps": params["input_time_steps"],
            "period": params["period"],
        })
    elif modelSelection == "Mamba":
        model_kwargs.update({
            "mamba_hidden_size": params["mamba_hidden_size"],
            "mamba_num_layers": params["mamba_num_layers"],
            "period": params["period"],
        })
    elif modelSelection == "Transformer":
        model_kwargs.update({
            "transformer_hidden_size": params["transformer_hidden_size"],
            "num_transformer_layers": params["num_transformer_layers"],
            "num_heads": params["num_heads"],
            "input_time_steps": params["input_time_steps"],
        })
    elif modelSelection == "KOSS":
        model_kwargs.update({
            "KOSS_hidden_size": params["KOSS_hidden_size"],
            "KOSS_num_layers": params["KOSS_num_layers"],
        })

    # å®ä¾‹åŒ–æ¨¡å‹å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
    model = ModelClass(**model_kwargs).to(device)

    print(f"âœ… å·²é€‰æ‹©æ¨¡å‹: {modelSelection}")
    return params, model, device


def model_searchBest(modelSelection, num_features):
    """æ ¹æ® modelSelection é€‰æ‹©å¹¶å®ä¾‹åŒ–ä¸åŒçš„æ¨¡å‹"""
    # è®¾å®šè®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # **æ¨¡å‹å­—å…¸ï¼šæ˜ å°„æ¨¡å‹åç§°åˆ° (æ¨¡å—å, å‚æ•°å˜é‡, æ¨¡å‹ç±»)**
    model_dict = {
        "LSTM": ("LSTMDefinite", "paramsLSTM", "LSTMModel"),
        "Mamba": ("MambaDefinite", "paramsMamba", "MambaModel"),
        "Transformer": ("TransformerDefine", "paramsTransformer", "TransformerModel"),
        "KOSS": ("KOSSDefiniteSix", "paramsKOSS", "KOSSModel"),
    }

    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆæ¨¡å‹
    if modelSelection not in model_dict:
        raise ValueError(f"âŒ æœªçŸ¥çš„æ¨¡å‹é€‰æ‹©: {modelSelection}")

    # è·å–æ¨¡å—ã€å‚æ•°ã€æ¨¡å‹åç§°
    module_name, params_name, model_name = model_dict[modelSelection]

    # åŠ¨æ€å¯¼å…¥æ¨¡å—
    module = __import__(module_name, fromlist=[params_name, model_name])

    # è·å–å‚æ•°å’Œæ¨¡å‹
    params = getattr(module, params_name)
    ModelClass = getattr(module, model_name)
    params["input_features"] = num_features
    params["output_features"] = num_features

    # é€šç”¨å‚æ•°è§£æ
    model_kwargs = {
        "input_features": params["input_features"],
        "output_time_steps": params["output_time_steps"],
        "output_features": params["output_features"],
    }

    # å„æ¨¡å‹çš„é¢å¤–å‚æ•°
    if modelSelection == "LSTM":
        model_kwargs.update({
            "lstm_hidden_size": params["lstm_hidden_size"],
            "lstm_num_layers": params["lstm_num_layers"],
            "input_time_steps": params["input_time_steps"],
            "period": params["period"],
        })
    elif modelSelection == "Mamba":
        model_kwargs.update({
            "mamba_hidden_size": params["mamba_hidden_size"],
            "mamba_num_layers": params["mamba_num_layers"],
            "period": params["period"],
        })
    elif modelSelection == "Transformer":
        model_kwargs.update({
            "transformer_hidden_size": params["transformer_hidden_size"],
            "num_transformer_layers": params["num_transformer_layers"],
            "num_heads": params["num_heads"],
            "input_time_steps": params["input_time_steps"],
        })
    elif modelSelection == "KOSS":
        model_kwargs.update({
            "KOSS_hidden_size": params["KOSS_hidden_size"],
            "KOSS_num_layers": params["KOSS_num_layers"],
        })

    # # å®ä¾‹åŒ–æ¨¡å‹å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
    # model = ModelClass(**model_kwargs).to(device)

    print(f"âœ… å·²é€‰æ‹©æ¨¡å‹: {modelSelection}")
    return params, ModelClass, device


def count_parameters(model):
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = total_params / 1e6
    trainable_params = trainable_params / 1e6

    print(f"Total parameters: {total_params:.2f}M")
    print(f"Trainable parameters: {trainable_params:.2f}M")

    return total_params, trainable_params


# é€‰æ‹©æ¨¡å‹
# modelSelection = "LSTM"  # å¯æ›´æ”¹ä¸º "Mamba" æˆ– "Transformer"æˆ–"Mamba"æˆ–"S6KF"
# modelSelection = "Mamba"  # å¯æ›´æ”¹ä¸º "Mamba" æˆ– "Transformer"æˆ–"Mamba"æˆ–"S6KF"
# modelSelection = "Transformer"  # å¯æ›´æ”¹ä¸º "Mamba" æˆ– "Transformer"æˆ–"Mamba"æˆ–"S6KF"
modelSelection = "KOSS"  # å¯æ›´æ”¹ä¸º "Mamba" æˆ– "Transformer"æˆ–"Mamba"æˆ–"S6KF"
datasetSelection = "ETT-small"  # exchange_rateã€ electricity, ETT-small, national_illness, weather, traffic


def main():
    vis_path, dataset_path, num_features = dataset_selection(datasetSelection)
    criterion = MSEHuberLoss()

    params, model, device = model_selective(modelSelection, num_features)
    train_loader, val_loader, test_loader, scaler = load_data(
        dataset_path, params["input_time_steps"], params["output_time_steps"], params["batch_size"]
    )

    # # å…¶ä»–ï¼Œæ¨¡å‹å‚æ•°è®¾ç½®
    # base_dir = './dataset'
    # cleaned_filename = f"{datasetSelection}_cleaned.csv"
    # cleaned_filepath = os.path.join(base_dir, cleaned_filename)
    #
    # #if modelSelection != "KOSS":
    # if not os.path.exists(cleaned_filepath):
    #     raise FileNotFoundError(f"âŒ æ¸…ç†åçš„æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°ï¼š{cleaned_filepath}")
    #
    # print(f"ğŸ“¥ æ­£åœ¨åŠ è½½æ•°æ®æ–‡ä»¶ï¼š{cleaned_filepath}")
    # df = pd.read_csv(cleaned_filepath)
    #
    # if df.shape[0] != 1440:
    #     raise ValueError(f"â— æ•°æ®è¡Œæ•°åº”ä¸º 1440ï¼Œä½†å½“å‰ä¸º {df.shape[0]}")
    #
    # # è½¬ä¸º numpy arrayï¼Œå¹¶æ‹†åˆ†
    # data = df.to_numpy(dtype=np.float32)  # shape: (1440, feature_dim)
    # inputs = data[:720]  # å‰ 720 æ­¥
    # targets = data[720:]  # å 720 æ­¥
    #
    # # [1, seq_len, feature_dim]
    # inputs = torch.tensor(inputs).unsqueeze(0)
    # targets = torch.tensor(targets).unsqueeze(0)
    #
    # # æ„é€  TensorDataset å’Œ DataLoader
    # dataset = TensorDataset(inputs, targets)
    # test_loader = DataLoader(dataset, batch_size=1, shuffle=False)
    #
    # print(f"âœ… DataLoader å·²åˆ›å»ºï¼Œè¾“å…¥ shape: {inputs.shape}ï¼Œç›®æ ‡ shape: {targets.shape}")
    # # else:
    # #     print("âœ… å½“å‰æ¨¡å‹ä¸º KOSSï¼Œè·³è¿‡æ•°æ®åŠ è½½ï¼ˆç”±ä¸»æ¡†æ¶å¤„ç†ï¼‰")

    best_mae = float("inf")
    best_mse = None
    best_train_losses = None
    best_samples = None
    total_mse, total_mae = 0, 0
    total_latency, total_throughput = 0, 0

    for run in range(3):  # è¿›è¡Œ4æ¬¡è®­ç»ƒ
        print(f"\n===== ç¬¬ {run + 1} æ¬¡è®­ç»ƒ =====\n")
        model.apply(lambda m: m.reset_parameters() if hasattr(m, "reset_parameters") else None)

        optimizer = torch.optim.Adam(model.parameters(), lr=params["learning_rate"], weight_decay=1e-3)
        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)

        # è®­ç»ƒ
        train_losses, _ = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler,
                                      params["num_epochs"], device)

        avg_latency, throughput = benchmark_throughput(model, test_loader, device='cuda')

        # æµ‹è¯•
        mse, mae, best_item = validate_model(model, test_loader, device)
        print(f"Run {run + 1} - Test MSE: {mse:.4f}, Test MAE: {mae:.4f}")
        total_mse += mse
        total_mae += mae
        total_latency += avg_latency
        total_throughput += throughput

        # # æ›´æ–°æœ€ä½³æ¨¡å‹
        if mae < best_mae:
            best_mae = mae
            best_mse = mse
            best_train_losses = train_losses
            best_samples = best_item

    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æœ€ä¼˜ MAE: {best_mae:.4f}ï¼Œå¯¹åº” MSE: {best_mse:.4f}")
    # ç”»æœ€ä¼˜æŸå¤±å‡½æ•°
    plot_training_losses(best_train_losses)

    best_mse = total_mse/3
    best_mae = total_mae/3
    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼å¹³å‡ MAE: {best_mae:.4f}ï¼Œå¯¹åº” MSE: {best_mse:.4f}")

    print(f"ğŸ“ˆ Avg Latency per batch: {total_latency / 3 * 1000:.2f} ms")
    print(f"âš¡ Avg Throughput: {total_throughput / 3:.2f} samples/s")
    # ä¿å­˜æœ€ä¼˜æ¨¡å‹
    save_training_results(
        model=model,
        optimizer=optimizer,
        num_epochs=params["num_epochs"],
        train_losses=best_train_losses,
        mse=best_mse,
        mae=best_mae,
        best_samples=best_samples,
        model_name=modelSelection,
        time_step=params["input_time_steps"],
        base_model_dir=vis_path,
        base_loss_dir=vis_path,
        base_metric_dir=vis_path,
        base_samples_dir=vis_path
    )


if __name__ == "__main__":
    main()
